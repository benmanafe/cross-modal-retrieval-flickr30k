# ğŸ” Cross-Modal Retrieval System â€“ Flickr30k

This project implements a **CLIP-style imageâ€“text retrieval system** trained from scratch on the **Flickr30k** dataset.  
It allows users to **search for images using text captions** or **find captions describing an uploaded image** â€” all powered by deep visualâ€“language embeddings and a Streamlit web app.

---

## ğŸ§  Overview

This system learns a **shared embedding space** between images and text.  
When you query with text or an image, it finds the most semantically similar counterparts based on **cosine similarity** of learned representations.

Key components:
- **Image Encoder:** EfficientNet-B0 (fine-tuned on Flickr30k)
- **Text Encoder:** MiniLM (Sentence-Transformers)
- **Loss Function:** Symmetric CLIP-style contrastive loss
- **Deployment:** Streamlit interactive interface

---

## ğŸš€ Features

âœ… **Text â†’ Image Retrieval** â€“ Enter any caption and find the top-10 most similar Flickr30k images.  
âœ… **Image â†’ Text Retrieval** â€“ Upload an image and see the top-10 matching captions.  
âœ… **Custom Fine-Tuned Model** â€“ Jointly trained EfficientNet & MiniLM encoders.  
âœ… **GPU-Accelerated Training** â€“ PyTorch implementation with AdamW optimizer.  
âœ… **Interactive Deployment** â€“ Streamlit frontend for local or online demo.

---

## ğŸ§© Project Structure

```
ğŸ“¦ cross-modal-retrieval-flickr30k/
â”œâ”€â”€ app.py                         # Streamlit web app
â”œâ”€â”€ model.py                       # Model definitions (ImageEncoder, TextEncoder, CrossModalModel)
â”œâ”€â”€ training_script.py              # Model training & fine-tuning code
â”œâ”€â”€ captions.txt                   # Flickr30k captions file
â”œâ”€â”€ flickr30k_images/              # Flickr30k image directory
â”œâ”€â”€ model-checkpoints/
â”‚   â”œâ”€â”€ model_epoch_10.pt
â”‚   â”œâ”€â”€ img_embeds_epoch_10.pt
â”‚   â”œâ”€â”€ txt_embeds_epoch_10.pt
â””â”€â”€ README.md                      # This file
```

---

## ğŸ§± Model Architecture

### ğŸ”¹ Image Encoder
- Backbone: `EfficientNet-B0`
- Output: 1280 â†’ 1024 linear projection  
- Normalized embeddings

### ğŸ”¹ Text Encoder
- Backbone: `MiniLM-L12-v2` (from `sentence-transformers`)
- Output: 384 â†’ 1024 linear projection  
- Mean pooling across token embeddings

### ğŸ”¹ Contrastive Loss (CLIP-style)
```python
loss = (CE(image_logits, labels) + CE(text_logits, labels)) / 2
```
Where logits are computed via scaled cosine similarity.

---

## ğŸ“ˆ Training Details

| Parameter | Value |
|------------|--------|
| Dataset | Flickr30k |
| Train Size | 145,000 image-caption pairs |
| Optimizer | AdamW |
| LR | 1e-4 (fine-tune: 5e-6) |
| Batch Size | 64 |
| Epochs | 10 |
| Embed Dim | 1024 |
| Loss | Symmetric Cross-Entropy |

During training, the model consistently reduced loss from **3.37 â†’ 1.48**, showing strong convergence.

---

## ğŸ’» Deployment (Streamlit)

Run locally:
```bash
pip install -r requirements.txt
streamlit run app.py
```

The app allows:
- Typing a text prompt to retrieve the **top 10 most relevant images**
- Uploading an image to retrieve **top 10 matching captions**

### Example UI

#### ğŸ—¨ï¸ Text â†’ Image
> â€œTwo men in green shirts drinkingâ€

â¡ Displays 10 similar Flickr30k images.

#### ğŸ–¼ï¸ Image â†’ Text
> Upload an image â†’ Returns top 10 captions ranked by similarity score.

---

## ğŸ“Š Results

**Validation Loss Progression:**
| Epoch | Train Loss | Val Loss |
|:------|-------------|-----------|
| 1 | 3.38 | 2.80 |
| 5 | 1.84 | 2.40 |
| 10 | 1.48 | 2.31 |

**Similarity Heatmap Example:**

A strong diagonal indicates learned alignment between images and captions.

![Cosine Similarity Matrix](assets/similarity_matrix.png)

---

## ğŸ§  Insights

- The model learns cross-modal alignment effectively but benefits from additional fine-tuning (smaller LR, larger batches).
- Averaging multiple captions per image improves retrieval stability.
- Embedding normalization is essential for consistent cosine similarity results.

---

## ğŸ§© Future Improvements

- [ ] Extend training with CLIP-like pretraining datasets (e.g., LAION)
- [ ] Add hard negative sampling
- [ ] Try larger embedding dimensions or ViT backbone
- [ ] Quantitative evaluation (Recall@K metrics)
- [ ] Deploy with FastAPI + Streamlit hybrid backend

---

## âš™ï¸ Requirements

```bash
torch>=2.0.0
torchvision>=0.15.0
transformers>=4.40.0
pandas
numpy
pillow
matplotlib
seaborn
tqdm
streamlit
```

---

## ğŸ§‘â€ğŸ’» Author

**Your Name**  
ğŸ“§ your.email@example.com  
ğŸ”— [LinkedIn Profile](https://linkedin.com/in/yourprofile)  
ğŸ’¼ [GitHub Repository](https://github.com/yourusername/cross-modal-retrieval-flickr30k)

---

## ğŸªª License
MIT License Â© 2025 Your Name.  
Feel free to use, modify, and distribute this project with attribution.

---

### â­ If you find this useful, consider giving the repo a star!
