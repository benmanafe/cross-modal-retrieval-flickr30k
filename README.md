# ğŸ” Cross-Modal Retrieval System â€“ Flickr30k

This project implements a **CLIP-style imageâ€“text retrieval system** trained from scratch on the **Flickr30k** dataset.
It allows users to **search for images using text captions** or **find captions describing an uploaded image** â€” all powered by deep visualâ€“language embeddings and a Streamlit web app.

---

## ğŸ§  Overview

This system learns a **shared embedding space** between images and text.
When you query with text or an image, it finds the most semantically similar counterparts based on **cosine similarity** of learned representations.

Key components:
* **Image Encoder:** EfficientNet-B0 (fine-tuned on Flickr30k)
* **Text Encoder:** MiniLM (Sentence-Transformers)
* **Loss Function:** Symmetric CLIP-style contrastive loss
* **Deployment:** Streamlit interactive interface

---

## ğŸš€ Features

âœ… **Text â†’ Image Retrieval** â€“ Enter any caption and find the top-10 most similar Flickr30k images.
âœ… **Image â†’ Text Retrieval** â€“ Upload an image and see the top-10 matching captions.
âœ… **Custom Fine-Tuned Model** â€“ Jointly trained EfficientNet & MiniLM encoders.
âœ… **GPU-Accelerated Training** â€“ PyTorch implementation with AdamW optimizer.
âœ… **Interactive Deployment** â€“ Streamlit frontend for local or online demo.

---

## ğŸ§© Project Structure
